# CUDA Learning Plan: Programming Massively Parallel Processors (4th Ed.)

**Plan Start Date:** August 6, 2025
**Weekly Time Commitment:** 3 hours (1-1.5 hrs reading + 1.5-2 hrs hands-on)
**Resource:** *Programming Massively Parallel Processors: A Hands-on Approach* (4th Edition)

This plan is designed to build a strong foundational understanding of CUDA programming, with a primary focus on hands-on application. By following this schedule, you will systematically cover key concepts while actively applying them through code.

---
| Week | Date Range | Chapters/Sections to Read | Weekly Goal | Learning Objectives | Hands-on Activities | Success Criteria |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **1** | Aug 6 - Aug 12 | **Chapter 1: A First Look at CUDA.** Read Sections 1.1-1.3. | Set up development environment and understand parallel computing basics. | Understand parallel vs. serial computing; learn about CPU/GPU differences; install CUDA Toolkit. | Install CUDA Toolkit; compile and run a "Hello, World!" CUDA program. | `nvcc` compiler is working; a simple CUDA program compiles and executes. |
| **2** | Aug 13 - Aug 19 | **Chapter 2: The CUDA Programming Model.** Read Sections 2.1-2.4. | Grasp the fundamental CUDA programming model. | Understand host/device distinction; learn kernel syntax; comprehend threads, blocks, and grids. | Modify "Hello, World!" to launch a kernel; experiment with launch configurations. | Can write a basic kernel and launch it; can vary blocks/threads and observe output. |
| **3** | Aug 20 - Aug 26 | **Chapter 3: CUDA Memory Model and Optimization.** Read Sections 3.1-3.2. | Implement a foundational parallel algorithm. | Understand CUDA memory types; learn memory management functions; write a complete CUDA program. | Implement a vector addition program with data transfers between host and device. | Vector addition program works correctly; can explain memory management functions. |
| **4** | Aug 27 - Sep 2 | **Review Week.** | Reinforce core concepts and solidify understanding of the first three weeks. | Debugging and problem-solving skills for basic CUDA; deeper understanding of kernel launch and memory. | Rerun all previous programs; create code comments explaining each part; practice finding and fixing a bug. | Can troubleshoot a simple CUDA program; have a clear mental model of host/device interaction. |
| **5** | Sep 3 - Sep 9 | **Chapter 4: Parallel Algorithm Patterns (I).** Read Sections 4.1-4.2 on Parallel Reduction. | Begin to think about performance with memory usage. | Differentiate global, shared, and constant memory; learn to use shared memory for optimization. | Modify a simple kernel to use shared memory; time and analyze the performance difference. | Can write a kernel using shared memory; can articulate its performance benefits. |
| **6** | Sep 10 - Sep 16 | **Chapter 4: Parallel Algorithm Patterns (I).** Read Section 4.3 on Parallel Prefix-Sum (Scan). | Implement a classic parallel algorithm: reduction. | Understand parallel reduction algorithm; learn about tree-based reduction and avoiding race conditions. | Implement a parallel reduction kernel; start with a naive version and then optimize it. | Reduction kernel correctly computes the final sum; can explain why a simple reduction would fail. |
| **7** | Sep 17 - Sep 23 | **Chapter 5: Parallel Algorithm Patterns (II).** Read Sections 5.1-5.3 on Parallel Sort. | Tackle another fundamental parallel algorithm: prefix-sum (scan). | Understand concept of parallel prefix-sum; learn the "hill-climb" or "up-sweep/down-sweep" algorithm. | Implement a simple prefix-sum kernel based on the book's description. | Kernel correctly computes prefix-sum; can explain high-level steps of the algorithm. |
| **8** | Sep 24 - Sep 30 | **Review Week.** | Reinforce understanding of memory optimization and parallel algorithm patterns. | Solidify knowledge of when to use different memory types; reinforce understanding of parallel reduction and scan. | Review your reduction and prefix-sum kernels; profile their performance; create a write-up of key takeaways. | Solid knowledge of when to use different memory types; understand performance benefits of shared memory. |
| **9** | Oct 1 - Oct 7 | **Chapter 6: Data Transfer and Multithreaded Processing.** Read Sections 6.1-6.2. | Explore the challenges of parallel sorting. | Understand complexity of parallel sorting; learn techniques like bitonic sort or merge sort. | Implement a small-scale parallel sorting algorithm on the GPU. | Kernel correctly sorts a small array; can describe why simple sorting is inefficient on GPUs. |
| **10** | Oct 8 - Oct 14 | **Chapter 7: GPU Memory and Optimization Techniques.** Read Sections 7.1-7.2. | Learn to use professional profiling tools. | Understand role of profiling; learn to use NVIDIA's profiling tools (e.g., NVIDIA Nsight) to analyze kernels. | Install and configure NVIDIA Nsight; profile one of your previous kernels. | Can profile a kernel and interpret output to identify at least one bottleneck. |
| **11** | Oct 15 - Oct 21 | **Chapter 8: Case Study: Image Processing.** Read Sections 8.1-8.2. | Understand asynchronous execution with streams. | Learn about CUDA streams; understand how `pinned` memory enables async transfers. | Modify a previous program to use CUDA streams; measure performance gains. | Program uses streams correctly; can explain how streams improve application throughput. |
| **12** | Oct 22 - Oct 28 | **Review Week.** | Synthesize knowledge of performance optimization; connect profiling, memory, and asynchronous execution. | Connect concepts of profiling, memory access, and async execution; think about real-world applications. | Revisit profiling results and apply an optimization; write a short summary of key optimizations. | Can connect concepts of profiling and optimization; have a good grasp of advanced topics. |
| **13** | Oct 29 - Nov 4 | **Chapter 9: Putting It All Together: A Case Study.** | Get acquainted with high-level libraries. | Understand usefulness of cuBLAS, cuDNN, Thrust; learn to use one for a common task. | Use a library like cuBLAS to perform matrix multiplication; compare code to a custom kernel. | Can successfully use a high-level library; can explain trade-offs. |
| **14** | Nov 5 - Nov 11 | **Chapter 10: Multithreaded Processing.** Read Sections 10.1-10.2. | Apply all knowledge to a comprehensive project. | Synthesize all concepts into a single application; practice designing a CUDA-accelerated solution. | Follow a case study from the book, or design a small project related to robotics. | Have a working, non-trivial CUDA program; can explain the design choices. |
| **15** | Nov 12 - Nov 18 | **Final Review & Next Steps.** | Consolidate learning and plan for the future. | Solidify mastery of CUDA fundamentals; understand how to continue learning and applying CUDA. | Rerun and optimize the final project; write a blog post summarizing the journey; brainstorm next steps. | Feel confident in ability to write and optimize CUDA code; have a clear vision for applying the skill. |
---

### Tips and Notes:

- **Consistency is Key:** The 3-hour per week commitment is designed to be sustainable. Stick to the schedule to build momentum.
- **Hands-on is Everything:** The `Hands-on Activities` are the most important part of this plan. Don't just read the code; write it, debug it, and experiment with it.
- **Review is Crucial:** The dedicated review weeks are for solidifying concepts. Use them to debug and document your code, ensuring you truly understand the "why" behind the "how."
- **Document Your Journey:** Consider writing brief notes or small blog posts after each week or major milestone. This helps reinforce learning and builds content for your personal brand.
- **Don't Be Afraid to Slow Down:** If a topic is particularly challenging, don't feel pressured to finish a section in a single week. It's better to understand a concept fully than to rush ahead.